{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path as osp\n",
    "import json\n",
    "from datetime import datetime\n",
    "from expense_analysis.tools import get_hash\n",
    "from loguru import logger\n",
    "\n",
    "from expense_analysis.correction import (\n",
    "    CategoryCorrectionWhereLabelContains,\n",
    "    CategoryCorrectionFromLoc,\n",
    "    DateCorrectionFromLoc,\n",
    "    RowDroppingFromLoc,\n",
    "    CorrectionSet\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data_version.json\") as f:\n",
    "    data_version = json.load(f)\n",
    "    \n",
    "with open(\"data_history.json\") as f:\n",
    "    data_history = json.load(f)\n",
    "    \n",
    "DATA_SUBDIR = osp.abspath(\"./datasets-history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_contains(s, keyword):\n",
    "    if pd.isna(s):\n",
    "        return False\n",
    "    else:\n",
    "        return keyword.lower() in s.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_COLS = [\n",
    "    \"date\",\n",
    "    \"bank_name\",\n",
    "    \"custom_account_name\",\n",
    "    \"label\",\n",
    "    \"amount\",\n",
    "    \"category_name\",\n",
    "    \"proportion\",\n",
    "    \"to_be_matched\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 22/08"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From updated Cozy Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_NAME = \"cozy_cloud\"\n",
    "raw_data_path = osp.abspath(\"../../perso/tmp/banks/export-data-banks-3-month-2024.csv\")\n",
    "df_new_cc_raw = pd.read_csv(raw_data_path, sep=\";\", decimal=\",\")\n",
    "df_new_cc = df_new_cc_raw.rename(columns={c: c.lower().replace(\" \", \"_\") for c in df_new_cc_raw.columns})\n",
    "df_new_cc[\"date\"] = pd.to_datetime(df_new_cc[\"date\"], format=\"%Y-%m-%d\")\n",
    "df_new_cc = df_new_cc.assign(proportion=1.0, shared=False)\n",
    "df_new_cc.sort_values(by=\"date\", inplace=True)\n",
    "df_new_cc[\"amount\"] = df_new_cc.amount.astype(float)\n",
    "df_new_cc = df_new_cc[np.abs(df_new_cc.amount) > 0.01]\n",
    "df_new_cc[\"to_be_matched\"] = False\n",
    "\n",
    "corrections_set = CorrectionSet.from_json(f\"corrections_{DF_NAME}.json\")\n",
    "\n",
    "corrections_set.apply(df_new_cc)\n",
    "\n",
    "df_new_cc = df_new_cc[\n",
    "    np.logical_or(\n",
    "        (\"2024-04-01\" <= df_new_cc.date) * (df_new_cc.date < \"2024-09-01\"),\n",
    "        (\"2022-04-01\" <= df_new_cc.date) * (df_new_cc.date < \"2022-09-01\"),\n",
    "    )\n",
    "]\n",
    "\n",
    "df_new_cc = df_new_cc[FINAL_COLS].reset_index(drop=True).reset_index()\n",
    "\n",
    "# check version\n",
    "DF_ = df_new_cc\n",
    "raw_data_path_entry_name = f\"{DF_NAME}_raw_data_path\"\n",
    "hash_entry_name = f\"{DF_NAME}_hash\"\n",
    "update_date_entry_name = f\"{DF_NAME}_last_update\"\n",
    "df_hash = get_hash(DF_)\n",
    "if df_hash != data_version.get(hash_entry_name) or raw_data_path != data_version.get(raw_data_path_entry_name):\n",
    "    logger.warning(\"Changes in df_new_cc: updating data_version.json\")\n",
    "    data_version[hash_entry_name] = df_hash\n",
    "    data_version[raw_data_path_entry_name] = raw_data_path\n",
    "    data_version[update_date_entry_name] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    with open(\"data_version.json\", \"w\") as f:\n",
    "        json.dump(data_version, f, indent=4)\n",
    "        \n",
    "    # update this dataset's version\n",
    "    DF_.to_csv(osp.join(DATA_SUBDIR, f\"{DF_NAME}_{df_hash}.csv\"), index=False)\n",
    "\n",
    "df_new_cc.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From food cards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EdenRed : no information on date of interrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_NAME = \"myedenred\"\n",
    "raw_data_path = osp.abspath(\"../../perso/tmp/banks/MyEdenRed-2023.csv\")\n",
    "df_edenred = pd.read_csv(raw_data_path, sep=\",\", decimal=\",\")\n",
    "df_edenred = df_edenred.drop(columns=[\"Unnamed: 0\", \"Statut\", \"Type\"])\n",
    "df_edenred = df_edenred.rename(columns={\"Date\": \"date\", \"Détails\": \"label\", \"Montant\": \"amount\"})\n",
    "df_edenred[\"date\"] = pd.to_datetime(df_edenred[\"date\"], format=\"%d/%m/%Y\")\n",
    "df_edenred[\"amount\"] = df_edenred.amount.apply(lambda x: x.replace(\"€\", \"\").replace(\",\", \".\")).astype(float)\n",
    "df_edenred[\"bank_name\"] = \"EdenRed\"\n",
    "df_edenred[\"custom_account_name\"] = \"EdenRed\"\n",
    "df_edenred[\"category_name\"] = \"restaurantsAndBars\"\n",
    "df_edenred[\"proportion\"] = 1.0\n",
    "df_edenred[\"to_be_matched\"] = True\n",
    "\n",
    "# pre fill some categories\n",
    "corrections_set = CorrectionSet.from_json(f\"corrections_{DF_NAME}.json\")\n",
    "corrections_set.apply(df_edenred)\n",
    "\n",
    "# duplicate it to insert equivalent \"salary\" entries\n",
    "df_edenred = pd.concat(\n",
    "    [\n",
    "        # df_edenred.assign(categrory_name=\"activityIncome\"),\n",
    "        df_edenred.assign(categrory_name=\"supermarket\", amount=-df_edenred.amount)\n",
    "    ]\n",
    ").sort_values(by=\"date\")\n",
    "\n",
    "# filter date of interrest\n",
    "df_edenred = df_edenred[\n",
    "    np.logical_or(\n",
    "        (\"2024-04-01\" <= df_edenred.date) * (df_edenred.date < \"2024-09-01\"),\n",
    "        (\"2022-04-01\" <= df_edenred.date) * (df_edenred.date < \"2022-09-01\"),\n",
    "    )\n",
    "]\n",
    "\n",
    "df_edenred = df_edenred[FINAL_COLS].reset_index(drop=True).reset_index()\n",
    "\n",
    "# check version\n",
    "DF_NAME = \"myedenred\"\n",
    "DF_ = df_edenred\n",
    "raw_data_path_entry_name = f\"{DF_NAME}_raw_data_path\"\n",
    "hash_entry_name = f\"{DF_NAME}_hash\"\n",
    "update_date_entry_name = f\"{DF_NAME}_last_update\"\n",
    "df_hash = get_hash(DF_.to_dict())\n",
    "if df_hash != data_version.get(hash_entry_name) or raw_data_path != data_version.get(raw_data_path_entry_name):\n",
    "    logger.warning(\"Changes in df_new_cc: updating data_version.json\")\n",
    "    data_version[hash_entry_name] = df_hash\n",
    "    data_version[raw_data_path_entry_name] = raw_data_path\n",
    "    data_version[update_date_entry_name] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    with open(\"data_version.json\", \"w\") as f:\n",
    "        json.dump(data_version, f, indent=4)\n",
    "        \n",
    "    # update this dataset's version\n",
    "    DF_.to_csv(osp.join(DATA_SUBDIR, f\"{DF_NAME}_{df_hash}.csv\"), index=False)\n",
    "\n",
    "try:\n",
    "    df_edenred.sample(3)\n",
    "except ValueError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Swile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_NAME = \"swile\"\n",
    "\n",
    "raw_data_path = osp.abspath(\"../../perso/tmp/banks/swile_2024_08.csv\")\n",
    "df_swile = pd.read_csv(raw_data_path, sep=\";\")\n",
    "df_swile[\"date\"] = pd.to_datetime(df_swile[\"date\"], format=\"%Y/%m/%d\")\n",
    "df_swile[\"bank_name\"] = \"Swile\"\n",
    "df_swile[\"custom_account_name\"] = \"Swile\"\n",
    "df_swile[\"proportion\"] = np.where(df_swile.category_name == \"supermarket\", 0.7, 1.0)\n",
    "\n",
    "df_swile[\"to_be_matched\"] = np.where(df_swile.amount < -25, True, False)\n",
    "\n",
    "df_swile = df_swile[\n",
    "    np.logical_or(\n",
    "        (\"2024-04-01\" <= df_swile.date) * (df_swile.date < \"2024-09-01\"),\n",
    "        (\"2022-04-01\" <= df_swile.date) * (df_swile.date < \"2022-09-01\"),\n",
    "    )\n",
    "]\n",
    "\n",
    "df_swile = df_swile[FINAL_COLS].reset_index(drop=True).reset_index()\n",
    "\n",
    "# check version\n",
    "DF_ = df_swile\n",
    "raw_data_path_entry_name = f\"{DF_NAME}_raw_data_path\"\n",
    "hash_entry_name = f\"{DF_NAME}_hash\"\n",
    "update_date_entry_name = f\"{DF_NAME}_last_update\"\n",
    "df_hash = get_hash(DF_.to_dict())\n",
    "if df_hash != data_version.get(hash_entry_name) or raw_data_path != data_version.get(raw_data_path_entry_name):\n",
    "    logger.warning(\"Changes in df_new_cc: updating data_version.json\")\n",
    "    data_version[hash_entry_name] = df_hash\n",
    "    data_version[raw_data_path_entry_name] = raw_data_path\n",
    "    data_version[update_date_entry_name] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    with open(\"data_version.json\", \"w\") as f:\n",
    "        json.dump(data_version, f, indent=4)\n",
    "        \n",
    "    # update this dataset's version\n",
    "    DF_.to_csv(osp.join(DATA_SUBDIR, f\"{DF_NAME}_{df_hash}.csv\"), index=False)\n",
    "\n",
    "df_swile.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Cospender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ\n",
    "DF_NAME = \"cospender\"\n",
    "corrections_set = CorrectionSet.from_json(f\"corrections_{DF_NAME}.json\")\n",
    "raw_data_path = \"/Users/francois.weber/perso/tmp/banks/cospender-Lionciole-2024-08.csv\"\n",
    "df_cospend_raw = pd.read_csv(raw_data_path, sep=\";\", decimal=\",\")\n",
    "\n",
    "df_cospend = df_cospend_raw.rename(columns={c: c.lower().replace(\" \", \"_\") for c in df_cospend_raw.columns})\n",
    "df_cospend[\"date\"] = pd.to_datetime(df_cospend[\"date\"], format=\"%Y%m%d\")\n",
    "df_cospend = df_cospend.sort_values(by=\"date\")\n",
    "df_cospend[\"amount\"] = -df_cospend[\"amount\"].abs()\n",
    "df_cospend[\"transaction\"] = \"Payed by: \" + df_cospend[\"member\"] + \" for \" + df_cospend[\"beneficiaries\"]\n",
    "\n",
    "df_cospend = df_cospend.rename(columns={\"description\": \"label\"}).assign(\n",
    "    bank_name=\"Cospender\", custom_account_name=df_cospend.member, category_name=None\n",
    ")\n",
    "df_cospend = df_cospend.assign(\n",
    "    bank_name=\"Cospender\",\n",
    "    custom_account_name=df_cospend.transaction,\n",
    "    category_name=None,\n",
    ")\n",
    "\n",
    "\n",
    "# try to auto guess repartition\n",
    "df_cospend[\"proportion\"] = 1.0\n",
    "# 70/30 before going to 5 rue des Juifs\n",
    "df_cospend.loc[(df_cospend.date < \"2022-09-01\") * (df_cospend.beneficiaries == \"All\"), \"proportion\"] = 0.7\n",
    "# 60/40 before going to 5 rue des Juifs\n",
    "df_cospend.loc[\n",
    "    (\"2023-02-01\" <= df_cospend.date) * (df_cospend.beneficiaries == \"All\"),\n",
    "    \"proportion\",\n",
    "] = 0.6\n",
    "\n",
    "df_cospend.loc[df_cospend.beneficiaries == \"François\", \"amount\"] = -df_cospend.loc[\n",
    "    df_cospend.beneficiaries == \"François\", \"amount\"\n",
    "]\n",
    "df_cospend.loc[df_cospend.beneficiaries == \"François\", \"category_name\"] = \"friendBorrowing\"\n",
    "df_cospend.loc[df_cospend.beneficiaries == \"louloute\", \"category_name\"] = \"friendBorrowing\"\n",
    "\n",
    "df_cospend = df_cospend[np.abs(df_cospend[\"amount\"]) > 0.01]\n",
    "\n",
    "# Correctionss\n",
    "corrections_set.apply(df_cospend)\n",
    "\n",
    "\n",
    "df_cospend = df_cospend.reset_index(drop=True).reset_index()\n",
    "\n",
    "### Huge manual corrections\n",
    "corrections_set = CorrectionSet.from_json(f\"corrections_{DF_NAME}.json\")\n",
    "corrections_set.apply(df_cospend)\n",
    "    \n",
    "    \n",
    "# every time I log an expenses in Cospender, I'm also charged from the bank. let's keep this one and remove the one in the bank account\n",
    "# BUT, sometimes the cospend amount does not match the real-life expense\n",
    "# SO, rather than erasing it bank line, just add a positive amount somewhere that will cancel the bank expense in a groupby\n",
    "df_cospend_cancel_bank_lines = df_cospend[(df_cospend.member == \"François\")].copy()\n",
    "df_cospend_cancel_bank_lines[\"bank_name\"] = \"cancel from Cospender\"\n",
    "df_cospend_cancel_bank_lines[\"custom_account_name\"] = \"Payed by François\"\n",
    "df_cospend_cancel_bank_lines[\"amount\"] = -df_cospend_cancel_bank_lines[\"amount\"]\n",
    "df_cospend_cancel_bank_lines[\"proportion\"] = 1.0\n",
    "\n",
    "# posterior modification\n",
    "df_cospend_cancel_bank_lines.loc[1094, \"amount\"] = 50 # and not 70\n",
    "\n",
    "df_cospend = pd.concat([df_cospend, df_cospend_cancel_bank_lines]).sort_values(by=\"date\")\n",
    "# No longer prepare a flag for expenses that should match something in my other accounts thanks to the trick above\n",
    "df_cospend[\"to_be_matched\"] = False\n",
    "\n",
    "# filter date of interrest\n",
    "df_cospend = df_cospend[\n",
    "    np.logical_or(\n",
    "        (\"2024-04-01\" <= df_cospend.date) * (df_cospend.date < \"2024-09-01\"),\n",
    "        (\"2022-04-01\" <= df_cospend.date) * (df_cospend.date < \"2022-09-01\"),\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "# # # \n",
    "df_cospend = df_cospend[FINAL_COLS].reset_index(drop=True).reset_index()\n",
    "\n",
    "\n",
    "# check version\n",
    "DF_ = df_cospend\n",
    "raw_data_path_entry_name = f\"{DF_NAME}_raw_data_path\"\n",
    "hash_entry_name = f\"{DF_NAME}_hash\"\n",
    "update_date_entry_name = f\"{DF_NAME}_last_update\"\n",
    "df_hash = get_hash(DF_.to_dict())\n",
    "if df_hash != data_version.get(hash_entry_name) or raw_data_path != data_version.get(raw_data_path_entry_name):\n",
    "    logger.warning(\"Changes in df_new_cc: updating data_version.json\")\n",
    "    data_version[hash_entry_name] = df_hash\n",
    "    data_version[raw_data_path_entry_name] = raw_data_path\n",
    "    data_version[update_date_entry_name] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    with open(\"data_version.json\", \"w\") as f:\n",
    "        json.dump(data_version, f, indent=4)\n",
    "        \n",
    "    # update this dataset's version\n",
    "    DF_.to_csv(osp.join(DATA_SUBDIR, f\"{DF_NAME}_{df_hash}.csv\"), index=False)\n",
    "\n",
    "df_cospend.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add recurring expenses from spreadsheet\n",
    "On 5 rue des Juifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rue des Juifs\n",
    "DF_NAME = \"recurring_expenses_juifs\"\n",
    "raw_data_path = \"/Users/francois.weber/code/expense-analysis/recurrent_expenses_rue_des_juifs.csv\"\n",
    "df_spreadsheet_juifs = pd.read_csv(raw_data_path)\n",
    "\n",
    "\n",
    "DF_ = df_spreadsheet_juifs\n",
    "raw_data_path_entry_name = f\"{DF_NAME}_raw_data_path\"\n",
    "hash_entry_name = f\"{DF_NAME}_hash\"\n",
    "update_date_entry_name = f\"{DF_NAME}_last_update\"\n",
    "df_hash = get_hash(DF_.to_dict())\n",
    "if df_hash != data_version.get(hash_entry_name) or raw_data_path != data_version.get(raw_data_path_entry_name):\n",
    "    logger.warning(\"Changes in df_new_cc: updating data_version.json\")\n",
    "    data_version[hash_entry_name] = df_hash\n",
    "    data_version[raw_data_path_entry_name] = raw_data_path\n",
    "    data_version[update_date_entry_name] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    with open(\"data_version.json\", \"w\") as f:\n",
    "        json.dump(data_version, f, indent=4)\n",
    "        \n",
    "    # update this dataset's version\n",
    "    DF_.to_csv(osp.join(DATA_SUBDIR, f\"{DF_NAME}_{df_hash}.csv\"), index=False)\n",
    "\n",
    "df_spreadsheet_juifs.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape recurring expenses from rue du jeu des enfants\n",
    "380€ was the sum of every components and it's possible to go finer grain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path = \"/Users/francois.weber/code/expense-analysis/reccurent_expenses_rue_du_jeu_des_enfants.csv\"\n",
    "df_spreadsheet_enfants = pd.read_csv(raw_data_path)\n",
    "\n",
    "\n",
    "DF_NAME = \"recurring_expenses_jeux_des_enfants\"\n",
    "DF_ = df_spreadsheet_enfants\n",
    "raw_data_path_entry_name = f\"{DF_NAME}_raw_data_path\"\n",
    "hash_entry_name = f\"{DF_NAME}_hash\"\n",
    "update_date_entry_name = f\"{DF_NAME}_last_update\"\n",
    "df_hash = get_hash(DF_.to_dict())\n",
    "if df_hash != data_version.get(hash_entry_name) or raw_data_path != data_version.get(raw_data_path_entry_name):\n",
    "    logger.warning(\"Changes in df_new_cc: updating data_version.json\")\n",
    "    data_version[hash_entry_name] = df_hash\n",
    "    data_version[raw_data_path_entry_name] = raw_data_path\n",
    "    data_version[update_date_entry_name] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    with open(\"data_version.json\", \"w\") as f:\n",
    "        json.dump(data_version, f, indent=4)\n",
    "        \n",
    "    # update this dataset's version\n",
    "    DF_.to_csv(osp.join(DATA_SUBDIR, f\"{DF_NAME}_{df_hash}.csv\"), index=False)\n",
    "\n",
    "df_spreadsheet_enfants.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path = None\n",
    "df = pd.concat(\n",
    "    [\n",
    "        df_new_cc,\n",
    "        df_cospend,\n",
    "        df_swile,\n",
    "        df_edenred,\n",
    "        df_spreadsheet_juifs,\n",
    "        df_spreadsheet_enfants,\n",
    "    ]\n",
    ")\n",
    "\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    \n",
    "df = (\n",
    "    df\n",
    "    .sort_values(by=\"date\")\n",
    "    .reset_index(drop=True)\n",
    "    .reset_index(names=\"uid\")\n",
    ")\n",
    "\n",
    "df[\"category_name\"] = df[\"category_name\"].str.strip()\n",
    "\n",
    "df = df[\n",
    "    np.logical_or(\n",
    "        (\"2024-04-01\" <= df.date) * (df.date < \"2024-09-01\"),\n",
    "        (\"2022-04-01\" <= df.date) * (df.date < \"2022-09-01\"),\n",
    "    )\n",
    "]\n",
    "\n",
    "with open(\"./cozy_categories_remapping.json\") as f:\n",
    "    category_mapping = json.load(f)\n",
    "    \n",
    "df = df.rename(columns={\"category_name\": \"old_category_name\"})\n",
    "df[\"main_category\"] = df[\"old_category_name\"].replace({k: v[\"mother_category\"] for k, v in category_mapping.items()})\n",
    "df[\"category_name\"] = df[\"old_category_name\"].replace({k: (v[\"mother_category\"] + \"-\" + v[\"renamed_category\"]) for k, v in category_mapping.items()})\n",
    "\n",
    "bank_name_to_exclude = [\"CCF\", \"La Banque Postale\"] \n",
    "df = df[~df.bank_name.isin(bank_name_to_exclude)]\n",
    "\n",
    "COMPRESS_DATASET = False\n",
    "if COMPRESS_DATASET:\n",
    "    # compress dataset\n",
    "    data = []\n",
    "    rows = df.to_dict(orient=\"records\")\n",
    "    prev_row = rows[0]\n",
    "    trailing = True\n",
    "    for row in df.to_dict(orient=\"records\")[1:]:\n",
    "        if row[\"uid\"] != prev_row[\"uid\"] and row.get(\"date\") == prev_row.get(\"date\") and row.get(\"bank_name\") == prev_row.get(\"bank_name\"):\n",
    "            prev_row[\"amount\"] += row[\"amount\"]\n",
    "            trailing = True\n",
    "        else:\n",
    "            data.append(prev_row)\n",
    "            prev_row = row\n",
    "            trailing = False\n",
    "    if trailing:\n",
    "        data.append(prev_row)\n",
    "        \n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "df[\"year\"] = df.date.dt.year\n",
    "df[\"month\"] = df.date.dt.month\n",
    "df[\"shared\"] = np.where(df.proportion < 1.0, \"share\", \"perso\")\n",
    "df[\"real_amount\"] = df.amount * df.proportion\n",
    "df[\"week\"] = df.date.dt.isocalendar().week\n",
    "\n",
    "DF_NAME = \"final_df\"\n",
    "DF_ = df\n",
    "raw_data_path_entry_name = f\"{DF_NAME}_raw_data_path\"\n",
    "hash_entry_name = f\"{DF_NAME}_hash\"\n",
    "update_date_entry_name = f\"{DF_NAME}_last_update\"\n",
    "df_hash = get_hash(DF_.to_dict())\n",
    "if df_hash != data_version.get(hash_entry_name) or raw_data_path != data_version.get(raw_data_path_entry_name):\n",
    "    logger.warning(f\"Changes in {DF_NAME}: updating data_version.json\")\n",
    "    data_version[hash_entry_name] = df_hash\n",
    "    data_version[raw_data_path_entry_name] = raw_data_path\n",
    "    data_version[update_date_entry_name] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    with open(\"data_version.json\", \"w\") as f:\n",
    "        json.dump(data_version, f, indent=4)\n",
    "        \n",
    "    # update this dataset's version\n",
    "    DF_.to_csv(osp.join(DATA_SUBDIR, f\"{DF_NAME}_{df_hash}.csv\"), index=False)\n",
    "\n",
    "    # maybe update history\n",
    "    previous_hash = data_history[\"version_history\"][0][hash_entry_name]\n",
    "    \n",
    "    data_history[\"version_history\"].insert(0, data_version)\n",
    "    with open(\"data_history.json\", \"w\") as f:\n",
    "        json.dump(data_history, f, indent=4)\n",
    "    logger.info(\"Dataset history updated\")\n",
    "\n",
    "df.to_csv(\"expenses-full-2022-2024.csv\", sep=\";\", index=False)\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedback"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "expense-analysis-rkvhOgC_-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
